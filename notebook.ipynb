{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LMSYS - Chatbot Arena Human Preference Predictions\n","\n","<div align=\"center\">\n","    <img src=\"https://i.ibb.co/wJMF5HL/lmsys.png\">\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["In this Kaggle challenge, our team will experiment with three different methods to determine which approach yields better results:\n","\n","1. **Method 1: Using Pretrained Gemma 2 Model**\n","   - Utilize the pretrained Gemma 2 model to leverage its capabilities for the task at hand.\n","\n","2. **Method 2: Using DeBERTa + TF-IDF + Word2Vec + Length + LightGBM**\n","   - Apply a combination of text vectorization techniques, including DeBERTa, TF-IDF, Word2Vec, and length features, along with LightGBM for model training and prediction.\n","\n","3. **Method 3: Combining Gemma and DeBERTa + TF-IDF + Word2Vec + LightGBM**\n","   - Combine the predictions from the Gemma model with those from the DeBERTa + TF-IDF + Word2Vec + LightGBM model to create a final prediction.\n","\n","By comparing these methods, we aim to identify which approach performs best for this challenge."]},{"cell_type":"markdown","metadata":{},"source":["# üìñ | Meta Data \n","\n","The competition dataset comprises user interactions from the ChatBot Arena. In each interaction, a judge presents one or more prompts to two different large language models and then indicates which model provided the more satisfactory response. The training data contains `55,000` rows, with an expected `25,000` rows in the test set.\n","\n","## Files\n","\n","### `train.csv`\n","- `id`: Unique identifier for each row.\n","- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.\n","- `prompt`: Input prompt given to both models.\n","- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n","- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).\n","\n","### `test.csv`\n","- `id`: Unique identifier for each row.\n","- `prompt`: Input prompt given to both models.\n","- `response_[a/b]`: Model_[a/b]'s response to the prompt."]},{"cell_type":"markdown","metadata":{},"source":["# Gemma 2 + Deberta + TF-IDF + Word2Vec + Length"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:53:15.523346Z","iopub.status.busy":"2024-08-24T12:53:15.522973Z","iopub.status.idle":"2024-08-24T12:53:15.528244Z","shell.execute_reply":"2024-08-24T12:53:15.527312Z","shell.execute_reply.started":"2024-08-24T12:53:15.523318Z"},"trusted":true},"outputs":[],"source":["# !pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n","# !pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n","# !pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n","# !pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:53:15.529958Z","iopub.status.busy":"2024-08-24T12:53:15.529538Z","iopub.status.idle":"2024-08-24T12:53:46.301383Z","shell.execute_reply":"2024-08-24T12:53:46.300485Z","shell.execute_reply.started":"2024-08-24T12:53:15.529908Z"},"trusted":true},"outputs":[],"source":["!pip install transformers peft accelerate bitsandbytes \\\n","    -U --no-index --find-links /kaggle/input/lmsys-wheel-files"]},{"cell_type":"markdown","metadata":{},"source":["# üìö | Import Libraries "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:53:46.303484Z","iopub.status.busy":"2024-08-24T12:53:46.303187Z","iopub.status.idle":"2024-08-24T12:54:03.637920Z","shell.execute_reply":"2024-08-24T12:54:03.637173Z","shell.execute_reply.started":"2024-08-24T12:53:46.303455Z"},"trusted":true},"outputs":[],"source":["from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\n","from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n","from peft import PeftModel\n","from dataclasses import dataclass\n","from concurrent.futures import ThreadPoolExecutor\n","import sklearn\n","from threading import Thread\n","import gc\n","import os\n","import io\n","import json\n","import random\n","import pickle\n","import zipfile\n","import datetime\n","import time\n","\n","import torch\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n","from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n","from torch.cuda.amp import autocast\n","from IPython.display import display\n","import torch.nn.functional as F\n","import tokenizers"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-24T12:54:03.639526Z","iopub.status.busy":"2024-08-24T12:54:03.638989Z","iopub.status.idle":"2024-08-24T12:54:28.168765Z","shell.execute_reply":"2024-08-24T12:54:28.167826Z","shell.execute_reply.started":"2024-08-24T12:54:03.639499Z"},"papermill":{"duration":4.432239,"end_time":"2024-05-07T00:30:21.264459","exception":false,"start_time":"2024-05-07T00:30:16.83222","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import regex as re\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, log_loss\n","from sklearn.preprocessing import LabelEncoder\n","from scipy.sparse import csr_matrix, save_npz, load_npz, hstack\n","\n","import lightgbm as lgb\n","\n","from tqdm import tqdm\n","\n","import gensim\n","import itertools\n","from gensim.utils import simple_preprocess\n","from gensim.models import Word2Vec\n","\n","from transformers import DebertaV2Tokenizer, DebertaV2Model\n","import torch\n","\n","import joblib\n","import unicodedata\n","import re\n","\n","import time\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Gemma Model Part"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:54:28.172132Z","iopub.status.busy":"2024-08-24T12:54:28.171426Z","iopub.status.idle":"2024-08-24T12:54:28.177753Z","shell.execute_reply":"2024-08-24T12:54:28.176778Z","shell.execute_reply.started":"2024-08-24T12:54:28.172096Z"},"trusted":true},"outputs":[],"source":["@dataclass\n","class Config:\n","    gemma_dir = '/kaggle/input/merged-v157-8bit'\n","    max_length = 3072\n","    batch_size = 2\n","    device = torch.device(\"cuda\")    \n","    tta = True  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n","    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n","\n","cfg = Config()"]},{"cell_type":"markdown","metadata":{},"source":["# üìÑ Load and Process Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:54:52.527948Z","iopub.status.busy":"2024-08-24T12:54:52.527572Z","iopub.status.idle":"2024-08-24T12:54:52.535856Z","shell.execute_reply":"2024-08-24T12:54:52.534965Z","shell.execute_reply.started":"2024-08-24T12:54:52.527917Z"},"trusted":true},"outputs":[],"source":["test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:54:55.023093Z","iopub.status.busy":"2024-08-24T12:54:55.022576Z","iopub.status.idle":"2024-08-24T12:54:55.033460Z","shell.execute_reply":"2024-08-24T12:54:55.031214Z","shell.execute_reply.started":"2024-08-24T12:54:55.023039Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","def process_text(text):\n","    return json.loads(text)\n","\n","test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n","test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n","test.loc[:, 'response_b'] = test['response_b'].apply(process_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:54:57.534999Z","iopub.status.busy":"2024-08-24T12:54:57.534675Z","iopub.status.idle":"2024-08-24T12:54:57.549048Z","shell.execute_reply":"2024-08-24T12:54:57.548079Z","shell.execute_reply.started":"2024-08-24T12:54:57.534973Z"},"trusted":true},"outputs":[],"source":["display(test.head(5))"]},{"cell_type":"markdown","metadata":{},"source":["# ‚úÇÔ∏è Tokenize Function For Gemma"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:54:58.696505Z","iopub.status.busy":"2024-08-24T12:54:58.695852Z","iopub.status.idle":"2024-08-24T12:54:58.702090Z","shell.execute_reply":"2024-08-24T12:54:58.701085Z","shell.execute_reply.started":"2024-08-24T12:54:58.696471Z"},"trusted":true},"outputs":[],"source":["# def tokenize(tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length):\n","#     # Handle different formats for different tokenizers\n","#     if isinstance(tokenizer, GemmaTokenizerFast):\n","#         prompt = [\"<prompt>: \" + p for p in prompt]\n","#         response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n","#         response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n","#     else:\n","#         prompt = [\"User prompt: \" + p for p in prompt]\n","#         response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n","#         response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n","    \n","#     # Tokenize with spread max length\n","#     if spread_max_length:\n","#         prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n","#         response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n","#         response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n","#         input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n","#         attention_mask = [[1] * len(i) for i in input_ids]\n","#     # Tokenize without spread max length\n","#     else:\n","#         text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n","#         tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n","#         input_ids = tokenized.input_ids\n","#         attention_mask = tokenized.attention_mask\n","    \n","#     return input_ids, attention_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:54:59.973152Z","iopub.status.busy":"2024-08-24T12:54:59.972528Z","iopub.status.idle":"2024-08-24T12:54:59.980964Z","shell.execute_reply":"2024-08-24T12:54:59.979937Z","shell.execute_reply.started":"2024-08-24T12:54:59.973121Z"},"trusted":true},"outputs":[],"source":["def tokenize(\n","    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n","):\n","\n","    text = []\n","    for pp,aa,bb in zip(prompt,response_a,response_b):\n","        \n","        rounds = [\n","            f\"<start_of_turn>prompt\\n{pp[i]}<end_of_turn>\\n\"\n","            +f\"<start_of_turn>response_a\\n{aa[i]}<end_of_turn>\\n\"\n","            +f\"<start_of_turn>response_b\\n{bb[i]}<end_of_turn>\"\n","            for i in range(len(pp))\n","        ]\n","        \n","        # CONCATENATE\n","        tmp = \"\\n\".join(rounds)\n","        for k in range(len(rounds)):\n","            tmp = \"\\n\".join(rounds[k:])\n","            if len( tokenizer(tmp)[\"input_ids\"] ) < max_length: \n","                break\n","        text.append( tmp )\n","\n","    tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n","    input_ids = tokenized.input_ids\n","    attention_mask = tokenized.attention_mask\n","    return input_ids, attention_mask"]},{"cell_type":"markdown","metadata":{},"source":["# üß† Tokenizer Setup and Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:55:01.428898Z","iopub.status.busy":"2024-08-24T12:55:01.428171Z","iopub.status.idle":"2024-08-24T12:55:02.516693Z","shell.execute_reply":"2024-08-24T12:55:02.515791Z","shell.execute_reply.started":"2024-08-24T12:55:01.428865Z"},"trusted":true},"outputs":[],"source":["%%time\n","\n","tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n","tokenizer.add_eos_token = True\n","tokenizer.padding_side = \"right\"\n","\n","data = pd.DataFrame()\n","data[\"id\"] = test[\"id\"]\n","data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n","data[\"length\"] = data[\"input_ids\"].apply(len)\n","\n","aug_data = pd.DataFrame()\n","aug_data[\"id\"] = test[\"id\"]\n","# swap response_a & response_b\n","aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n","aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:55:02.856898Z","iopub.status.busy":"2024-08-24T12:55:02.855972Z","iopub.status.idle":"2024-08-24T12:55:02.861125Z","shell.execute_reply":"2024-08-24T12:55:02.859882Z","shell.execute_reply.started":"2024-08-24T12:55:02.856863Z"},"trusted":true},"outputs":[],"source":["print(tokenizer.decode(data[\"input_ids\"][0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:55:03.051205Z","iopub.status.busy":"2024-08-24T12:55:03.050561Z","iopub.status.idle":"2024-08-24T12:55:03.055162Z","shell.execute_reply":"2024-08-24T12:55:03.054216Z","shell.execute_reply.started":"2024-08-24T12:55:03.051172Z"},"trusted":true},"outputs":[],"source":["print(tokenizer.decode(aug_data[\"input_ids\"][0]))"]},{"cell_type":"markdown","metadata":{},"source":["# ü§ñ | Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:55:03.391445Z","iopub.status.busy":"2024-08-24T12:55:03.390810Z","iopub.status.idle":"2024-08-24T12:56:32.740336Z","shell.execute_reply":"2024-08-24T12:56:32.739465Z","shell.execute_reply.started":"2024-08-24T12:55:03.391401Z"},"trusted":true},"outputs":[],"source":["# Load base model on GPU 0\n","device_0 = torch.device('cuda:0')\n","model_0 = Gemma2ForSequenceClassification.from_pretrained(\n","    cfg.gemma_dir,\n","    device_map=device_0,\n","    use_cache=False,\n",")\n","\n","# Load base model on GPU 1\n","device_1 = torch.device('cuda:1')\n","model_1 = Gemma2ForSequenceClassification.from_pretrained(\n","    cfg.gemma_dir,\n","    device_map=device_1,\n","    use_cache=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:32.742104Z","iopub.status.busy":"2024-08-24T12:56:32.741806Z","iopub.status.idle":"2024-08-24T12:56:32.746042Z","shell.execute_reply":"2024-08-24T12:56:32.745078Z","shell.execute_reply.started":"2024-08-24T12:56:32.742080Z"},"trusted":true},"outputs":[],"source":["model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n","model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)"]},{"cell_type":"markdown","metadata":{},"source":["# üîç Inference Function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:32.747694Z","iopub.status.busy":"2024-08-24T12:56:32.747356Z","iopub.status.idle":"2024-08-24T12:56:32.757843Z","shell.execute_reply":"2024-08-24T12:56:32.756948Z","shell.execute_reply.started":"2024-08-24T12:56:32.747670Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","@torch.cuda.amp.autocast()\n","def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n","    a_win, b_win, tie = [], [], []\n","    \n","    for start_idx in range(0, len(df), batch_size):\n","        end_idx = min(start_idx + batch_size, len(df))\n","        tmp = df.iloc[start_idx:end_idx]\n","        input_ids = tmp[\"input_ids\"].to_list()\n","        attention_mask = tmp[\"attention_mask\"].to_list()\n","        inputs = pad_without_fast_tokenizer_warning(\n","            tokenizer,\n","            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n","            padding=\"longest\",\n","            pad_to_multiple_of=None,\n","            return_tensors=\"pt\",\n","        )\n","        outputs = model(**inputs.to(device))\n","        proba = outputs.logits.softmax(-1).cpu()\n","        \n","        a_win.extend(proba[:, 0].tolist())\n","        b_win.extend(proba[:, 1].tolist())\n","        tie.extend(proba[:, 2].tolist())\n","    \n","    df[\"winner_model_a\"] = a_win\n","    df[\"winner_model_b\"] = b_win\n","    df[\"winner_tie\"] = tie\n","    \n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["# ‚ö° Perform Inference Using Threading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:32.760745Z","iopub.status.busy":"2024-08-24T12:56:32.760209Z","iopub.status.idle":"2024-08-24T12:56:37.348032Z","shell.execute_reply":"2024-08-24T12:56:37.346918Z","shell.execute_reply.started":"2024-08-24T12:56:32.760713Z"},"trusted":true},"outputs":[],"source":["st = time.time()\n","\n","# sort by input length to fully leverage dynaminc padding\n","data = data.sort_values(\"length\", ascending=False)\n","# the total #tokens in sub_1 and sub_2 should be more or less the same\n","sub_1 = data.iloc[0::2].copy()\n","sub_2 = data.iloc[1::2].copy()\n","\n","with ThreadPoolExecutor(max_workers=2) as executor:\n","    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n","\n","result_df = pd.concat(list(results), axis=0)\n","proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n","\n","print(f\"elapsed time: {time.time() - st}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:37.349718Z","iopub.status.busy":"2024-08-24T12:56:37.349388Z","iopub.status.idle":"2024-08-24T12:56:41.005545Z","shell.execute_reply":"2024-08-24T12:56:41.004512Z","shell.execute_reply.started":"2024-08-24T12:56:37.349691Z"},"trusted":true},"outputs":[],"source":["st = time.time()\n","\n","if cfg.tta:\n","    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n","    sub_1 = data.iloc[0::2].copy()\n","    sub_2 = data.iloc[1::2].copy()\n","\n","    with ThreadPoolExecutor(max_workers=2) as executor:\n","        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n","\n","    tta_result_df = pd.concat(list(results), axis=0)\n","    # recall TTA's order is flipped\n","    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n","    # average original result and TTA result.\n","    proba = (proba + tta_proba) / 2\n","\n","print(f\"elapsed time: {time.time() - st}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:41.007059Z","iopub.status.busy":"2024-08-24T12:56:41.006708Z","iopub.status.idle":"2024-08-24T12:56:41.021470Z","shell.execute_reply":"2024-08-24T12:56:41.020457Z","shell.execute_reply.started":"2024-08-24T12:56:41.007006Z"},"trusted":true},"outputs":[],"source":["result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n","result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n","result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n","gemma_result_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n","# gemma_result_df.to_csv('submission.csv', index=False)\n","display(gemma_result_df)"]},{"cell_type":"markdown","metadata":{},"source":["# Save gemma result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:41.023165Z","iopub.status.busy":"2024-08-24T12:56:41.022805Z","iopub.status.idle":"2024-08-24T12:56:41.029749Z","shell.execute_reply":"2024-08-24T12:56:41.028756Z","shell.execute_reply.started":"2024-08-24T12:56:41.023137Z"},"trusted":true},"outputs":[],"source":["TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n","\n","gemma_preds = result_df[TARGETS].values"]},{"cell_type":"markdown","metadata":{},"source":["# ü§ñ | Deberta + TF-IDF + Word2Vec + Length Model Part"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:41.031580Z","iopub.status.busy":"2024-08-24T12:56:41.031237Z","iopub.status.idle":"2024-08-24T12:56:44.564573Z","shell.execute_reply":"2024-08-24T12:56:44.563602Z","shell.execute_reply.started":"2024-08-24T12:56:41.031554Z"},"papermill":{"duration":4.40411,"end_time":"2024-05-07T00:30:25.698148","exception":false,"start_time":"2024-05-07T00:30:21.294038","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n","test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")\n","\n","vectorize_on_train_and_test = False\n","\n","#quick_test for training on small part of train data (and not using bunch of GPU on submit)\n","#(if this is on - saved models won't be fully trained)\n","quick_test = True\n","quick_test_items = 800\n","\n","#automatically disable quick_test if we detect actual test data... (assures full training when scoring)\n","if (len(test)) > 3:quick_test = False\n","    \n","if quick_test: train = train.head(quick_test_items)\n","    \n","target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n","\n","columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n","\n","train.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:44.565915Z","iopub.status.busy":"2024-08-24T12:56:44.565651Z","iopub.status.idle":"2024-08-24T12:56:46.259310Z","shell.execute_reply":"2024-08-24T12:56:46.258353Z","shell.execute_reply.started":"2024-08-24T12:56:44.565892Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n","test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")\n","vectorize_on_train_and_test = False\n","#quick_test for training on small part of train data (and not using bunch of GPU on submit)\n","#(if this is on - saved models won't be fully trained)\n","quick_test = True\n","quick_test_items = 1000\n","#automatically disable quick_test if we detect actual test data... (assures full training when scoring)\n","if (len(test)) > 3:quick_test = False\n","if quick_test: train = train.head(quick_test_items)\n","\n","def process(input_str):\n","    stripped_str = input_str.strip('[]')\n","    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n","    return  ' '.join(sentences)\n","test.loc[:, 'prompt'] = test['prompt'].apply(process)\n","test.loc[:, 'response_a'] = test['response_a'].apply(process)\n","test.loc[:, 'response_b'] = test['response_b'].apply(process)\n","train.loc[:, 'prompt'] = train['prompt'].apply(process)\n","train.loc[:, 'response_a'] = train['response_a'].apply(process)\n","train.loc[:, 'response_b'] = train['response_b'].apply(process)\n","\n","target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n","columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n","train['label'] = train[target_columns].idxmax(axis=1) \n","label_encoder = LabelEncoder()\n","train['label'] = label_encoder.fit_transform(train['label'])\n","train = train[columns_to_vectorize + ['label']]\n","train.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["# Deberta: Function to extract text features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:46.263788Z","iopub.status.busy":"2024-08-24T12:56:46.263495Z","iopub.status.idle":"2024-08-24T12:56:50.511795Z","shell.execute_reply":"2024-08-24T12:56:50.510775Z","shell.execute_reply.started":"2024-08-24T12:56:46.263762Z"},"trusted":true},"outputs":[],"source":["deberta_path = \"/kaggle/input/debertav3base\"\n","\n","# Load DeBERTa tokenizer and model\n","tokenizer = DebertaV2Tokenizer.from_pretrained(deberta_path)\n","transformer_model = DebertaV2Model.from_pretrained(deberta_path).cuda()\n","\n","# Function to extract features (got best performance of batch_size 2-3 range)\n","def batch_extract_transformer_features(texts, tokenizer, model, batch_size=2, max_length=1024):\n","    total_texts = 0\n","    total_over_max_length = 0\n","    \n","    features = []\n","    model.eval()  # Set model to evaluation mode\n","    # Use autocast for mixed precision\n","    with torch.cuda.amp.autocast():\n","        for i in range(0, len(texts), batch_size):\n","            batch_texts = texts[i:i+batch_size]\n","            inputs = tokenizer(batch_texts, return_tensors='pt', max_length=max_length, truncation=True, padding=True).to('cuda')\n","            \n","            input_ids = inputs['input_ids']\n","            # Check for truncation\n","            for j, input_id in enumerate(input_ids):\n","                total_texts += 1\n","                if (input_id == tokenizer.pad_token_id).sum() == 0 and input_id.shape[0] == max_length:\n","                    total_over_max_length +=1\n","            \n","            with torch.no_grad():\n","                outputs = model(**inputs)\n","            batch_features = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","            features.extend(batch_features)\n","            # clear cache / print status \".\"\n","            if i % (batch_size * 10) == 0:\n","                torch.cuda.empty_cache()\n","                print(\".\", end=\"\")\n","                \n","    print (\"Ratio of texts over max_length tokens:\", total_over_max_length / total_texts)\n","    return np.vstack(features)"]},{"cell_type":"markdown","metadata":{},"source":["# Deberta: Extract features for prompt and both responses\n","* Also adding difference between two responses to array (seemed to help)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:56:50.513394Z","iopub.status.busy":"2024-08-24T12:56:50.513090Z","iopub.status.idle":"2024-08-24T12:58:06.520028Z","shell.execute_reply":"2024-08-24T12:58:06.519095Z","shell.execute_reply.started":"2024-08-24T12:56:50.513370Z"},"trusted":true},"outputs":[],"source":["def get_transformer_vectors(df):\n","    vectors = []\n","    for column in tqdm(columns_to_vectorize, desc=\"Vectorizing Columns\"):\n","        print(\"Vectorizing\", column)\n","        vectors.append(batch_extract_transformer_features(df[column].tolist(), tokenizer, transformer_model))\n","\n","    vectors = np.array(vectors)\n","    vectors = np.transpose(vectors, (1, 0, 2))\n","\n","    # Compute average difference\n","    avg_dif = vectors[:, 1, :] - vectors[:, 2, :]\n","    avg_dif = avg_dif.reshape(vectors.shape[0], 1, vectors.shape[2])\n","    vectors = np.concatenate((vectors, avg_dif), axis=1)\n","\n","    # Calculate cosine similarities and append them\n","    similarities = []\n","    for i in range(vectors.shape[0]):\n","        prompt_vec = vectors[i, 0, :].reshape(1, -1)\n","        response1_vec = vectors[i, 1, :].reshape(1, -1)\n","        response2_vec = vectors[i, 2, :].reshape(1, -1)\n","        \n","        # Cosine similarity between prompt and response1\n","        sim_prompt_resp1 = cosine_similarity(prompt_vec, response1_vec)[0][0]\n","        \n","        # Cosine similarity between prompt and response2\n","        sim_prompt_resp2 = cosine_similarity(prompt_vec, response2_vec)[0][0]\n","        \n","        # Cosine similarity between response1 and response2\n","        sim_resp1_resp2 = cosine_similarity(response1_vec, response2_vec)[0][0]\n","        \n","        similarities.append([sim_prompt_resp1, sim_prompt_resp2, sim_resp1_resp2])\n","    \n","    similarities = np.array(similarities)\n","    \n","    # Reshape vectors to 2D\n","    vectors = vectors.reshape(len(vectors), -1)\n","    \n","    # Concatenate vectors and similarities\n","    final_vectors = np.concatenate((vectors, similarities), axis=1)\n","    \n","    return final_vectors\n","\n","start_time = time.time()\n","transformer_train_vectors = get_transformer_vectors(train)\n","print(time.time() - start_time, \"seconds\")\n","transformer_train_vectors.shape"]},{"cell_type":"markdown","metadata":{},"source":["# Define text for Word2Vec and TF-IDF Vectorizer training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:06.522059Z","iopub.status.busy":"2024-08-24T12:58:06.521424Z","iopub.status.idle":"2024-08-24T12:58:06.540764Z","shell.execute_reply":"2024-08-24T12:58:06.539745Z","shell.execute_reply.started":"2024-08-24T12:58:06.522007Z"},"trusted":true},"outputs":[],"source":["train_text = train[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)\n","test_text = test[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)\n","\n","if vectorize_on_train_and_test:\n","    vector_fit_text = pd.concat([train_text, test_text], axis=0).reset_index(drop=True)\n","else:\n","    vector_fit_text = train_text"]},{"cell_type":"markdown","metadata":{},"source":["# Word2Vec: Initialize / train on our train data...\n","* Here we train Word2Vec to capture word relationships on our text columns...."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:06.542240Z","iopub.status.busy":"2024-08-24T12:58:06.541926Z","iopub.status.idle":"2024-08-24T12:58:09.465916Z","shell.execute_reply":"2024-08-24T12:58:09.465070Z","shell.execute_reply.started":"2024-08-24T12:58:06.542215Z"},"trusted":true},"outputs":[],"source":["print(\"Training Word2Vec...\")\n","train_tokens = vector_fit_text.map(simple_preprocess)\n","\n","#performance vector_size much better at 60 than 150\n","vectors = Word2Vec(train_tokens, vector_size=60, window=3, seed=1, workers=4)\n","vectors.save(\"word2vec_trained.model\")\n","\n","print(\"Done.\")"]},{"cell_type":"markdown","metadata":{},"source":["# Word2Vec: Function to return average, min and max vectors for a text body\n","* Word2Vec provides vector values for each word - but we need a single vector that represents the entire text\n","* We do this by taking the average vector for all words in the text\n","* We also can return the minimum / maximum values across all vector components"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:09.467724Z","iopub.status.busy":"2024-08-24T12:58:09.467299Z","iopub.status.idle":"2024-08-24T12:58:09.478819Z","shell.execute_reply":"2024-08-24T12:58:09.477933Z","shell.execute_reply.started":"2024-08-24T12:58:09.467689Z"},"trusted":true},"outputs":[],"source":["def get_w2v_doc_vector(model, tokens, mode = \"mean\"):\n","    def doc_vector(words):\n","        vectors_in_doc = [model.wv[w] for w in words if w in model.wv]\n","        if (len(vectors_in_doc) == 0): return np.zeros(vectors.vector_size)\n","        if (mode == \"mean\"): return np.mean(vectors_in_doc, axis=0)\n","        if (mode == \"min\"): return np.min(vectors_in_doc, axis=0)\n","        if (mode == \"max\"): return np.max(vectors_in_doc, axis=0)\n","\n","    def replace_nan_with_default(x, default_vector):\n","        return default_vector if np.isnan(x).any() else x\n","\n","    X = tokens.map(doc_vector)\n","\n","    #default vector is average of all\n","    default_vector = X[ False == X.isnull() ].mean()\n","    \n","    return np.stack([replace_nan_with_default(vector, default_vector) for vector in X])"]},{"cell_type":"markdown","metadata":{},"source":["# Word2Vec: Vectorize prompt and both responses \n","* Word2Vec is used to generate mean, min and max vectors for the prompt and both responses\n","* We additionally generate a column with vector that's the difference between the two prompts\n","* Generating vectors with the differences between the prompt and responses didn't help score "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:09.480402Z","iopub.status.busy":"2024-08-24T12:58:09.480077Z","iopub.status.idle":"2024-08-24T12:58:15.316564Z","shell.execute_reply":"2024-08-24T12:58:15.315601Z","shell.execute_reply.started":"2024-08-24T12:58:09.480371Z"},"trusted":true},"outputs":[],"source":["def get_word2vec_vectors(df):\n","    word2vec_vectors = []\n","    for column in tqdm(columns_to_vectorize, desc=\"Vectorizing Columns\"):\n","        print(\"Vectorizing\", column)\n","        column_tokens = df[column].map(simple_preprocess)\n","\n","        word2vec_vectors.append(get_w2v_doc_vector(vectors, column_tokens, mode=\"mean\"))\n","        word2vec_vectors.append(get_w2v_doc_vector(vectors, column_tokens, mode=\"min\"))\n","        word2vec_vectors.append(get_w2v_doc_vector(vectors, column_tokens, mode=\"max\"))\n","\n","    #adjust array config\n","    word2vec_vectors = np.array(word2vec_vectors)\n","    word2vec_vectors = np.transpose(word2vec_vectors, (1, 0, 2))\n"," \n","    #generate a vector that is response_a - response_b (means values) / append it to array\n","    avg_dif = word2vec_vectors[:, 3, :] - word2vec_vectors[:, 6, :]\n","    avg_dif = avg_dif.reshape(word2vec_vectors.shape[0], 1, word2vec_vectors.shape[2])\n","    word2vec_vectors = np.concatenate((word2vec_vectors, avg_dif), axis=1)\n","    \n","    #flatten\n","    word2vec_vectors = np.array(word2vec_vectors).reshape(len(word2vec_vectors), -1)\n","    return word2vec_vectors\n","\n","word2vec_train_vectors = get_word2vec_vectors(train)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011059,"end_time":"2024-05-07T00:30:25.784583","exception":false,"start_time":"2024-05-07T00:30:25.773524","status":"completed"},"tags":[]},"source":["# TF-IDF: Fit vectorizer on prompts and both responses"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:15.318345Z","iopub.status.busy":"2024-08-24T12:58:15.317905Z","iopub.status.idle":"2024-08-24T12:58:46.127213Z","shell.execute_reply":"2024-08-24T12:58:46.126228Z","shell.execute_reply.started":"2024-08-24T12:58:15.318312Z"},"papermill":{"duration":820.367888,"end_time":"2024-05-07T00:44:06.162572","exception":false,"start_time":"2024-05-07T00:30:25.794684","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Define a named tokenizer function\n","#produces better results than using \"word\" for analyzer\n","def custom_tokenizer(text):\n","    return re.findall(r'[^\\W]+', text)\n","\n","#word-level vectorizer\n","tfidf_word_vectorizer = TfidfVectorizer(\n","    ngram_range=(1, 5),\n","    tokenizer=custom_tokenizer,\n","    token_pattern=None,\n","    strip_accents='unicode',\n","    min_df=4,\n","    max_features=300\n",")\n","\n","#char-level vectorizer\n","tfidf_char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 5), max_features=1000, min_df=4)\n","\n","def batch_process(texts, batch_size):\n","    for i in range(0, len(texts), batch_size):\n","        yield texts[i:i + batch_size]\n","\n","#doing in batches so we can see progress\n","batch_size = 1000\n","for batch in tqdm(batch_process(vector_fit_text, batch_size), total=np.ceil(len(vector_fit_text) / batch_size)):\n","    if len(batch) >= tfidf_word_vectorizer.min_df:\n","        tfidf_word_vectorizer.fit(batch)\n","    if len(batch) >= tfidf_char_vectorizer.min_df:\n","        tfidf_char_vectorizer.fit(batch)\n","        \n","joblib.dump(tfidf_word_vectorizer, \"tfidf_word_vectorizer.pkl\")\n","joblib.dump(tfidf_char_vectorizer, \"tfidf_char_vectorizer.pkl\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009716,"end_time":"2024-05-07T00:44:06.182403","exception":false,"start_time":"2024-05-07T00:44:06.172687","status":"completed"},"tags":[]},"source":["# TF-IDF: Vectorize text columns - and combine in hstack"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:46.128838Z","iopub.status.busy":"2024-08-24T12:58:46.128479Z","iopub.status.idle":"2024-08-24T12:58:58.451989Z","shell.execute_reply":"2024-08-24T12:58:58.451007Z","shell.execute_reply.started":"2024-08-24T12:58:46.128805Z"},"papermill":{"duration":220.241247,"end_time":"2024-05-07T00:47:46.433599","exception":false,"start_time":"2024-05-07T00:44:06.192352","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def get_tfidf_vectors(df):\n","    vectorized_columns = []\n","    for column in columns_to_vectorize:\n","        vectorized_columns.append(tfidf_word_vectorizer.transform(df[column]))\n","        vectorized_columns.append(tfidf_char_vectorizer.transform(df[column]))\n","    return hstack(vectorized_columns)\n","\n","tfidf_train_vectors = get_tfidf_vectors(train)"]},{"cell_type":"markdown","metadata":{},"source":["# Get Length Features\n","* Adapted from https://www.kaggle.com/code/currypurin/lmsys-lengthfeature-and-tf-idf-v2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:58.454087Z","iopub.status.busy":"2024-08-24T12:58:58.453748Z","iopub.status.idle":"2024-08-24T12:58:58.459205Z","shell.execute_reply":"2024-08-24T12:58:58.458269Z","shell.execute_reply.started":"2024-08-24T12:58:58.454056Z"},"trusted":true},"outputs":[],"source":["def has_none(vals) -> int:\n","    # some responses contains null and probably they are useful for prediction\n","    return int(any(val is None for val in vals))\n","\n","\n","def str_length(vals) -> int:\n","    length = 0\n","    for val in vals:\n","        if isinstance(val, str):\n","            length += len(val)\n","    return length\n","\n","\n","def get_length_features(data: pd.DataFrame):\n","    length_feature_array = []\n","    length_feature_array.append(data[\"response_a\"].apply(str_length))\n","    length_feature_array.append(data[\"response_b\"].apply(str_length))\n","    \n","    length_feature_array.append(length_feature_array[0] - length_feature_array[1])\n","    length_feature_array.append((length_feature_array[0] + length_feature_array[1]) / 2)\n","    length_feature_array.append((length_feature_array[0] / length_feature_array[1]))\n","    \n","    length_feature_array.append(data[\"response_a\"].apply(has_none))\n","    length_feature_array.append(data[\"response_b\"].apply(has_none))\n","    length_feature_array.append(data[\"response_a\"].apply(has_none) - data[\"response_b\"].apply(has_none))\n","    length_feature_array = np.array(length_feature_array).reshape(len(length_feature_array), -1)\n","    length_feature_array = np.transpose(length_feature_array, (1, 0))\n","\n","    return length_feature_array\n","\n","train_length_features = get_length_features(train)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Deberta + Word2Vec + TF-IDF + Length Features: Assemble vectors!\n","* Also saving out to disk..."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:58.461000Z","iopub.status.busy":"2024-08-24T12:58:58.460537Z","iopub.status.idle":"2024-08-24T12:58:59.871840Z","shell.execute_reply":"2024-08-24T12:58:59.871029Z","shell.execute_reply.started":"2024-08-24T12:58:58.460941Z"},"trusted":true},"outputs":[],"source":["transformer_train_vectors_csr = csr_matrix(transformer_train_vectors)\n","word2vec_train_vectors_csr = csr_matrix(word2vec_train_vectors)  \n","train_length_features_csr = csr_matrix(train_length_features)\n","\n","#save all the components\n","save_npz(os.path.join(\".\", 'transformer_train_vectors.npz'), transformer_train_vectors_csr)\n","save_npz(os.path.join(\".\", 'word2vec_train_vectors.npz'), word2vec_train_vectors_csr)\n","save_npz(os.path.join(\".\", 'train_length_features.npz'), train_length_features_csr)\n","\n","combined_train_vectors = hstack([tfidf_train_vectors, transformer_train_vectors, word2vec_train_vectors_csr, train_length_features_csr]) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:59.873332Z","iopub.status.busy":"2024-08-24T12:58:59.873010Z","iopub.status.idle":"2024-08-24T12:58:59.877730Z","shell.execute_reply":"2024-08-24T12:58:59.876674Z","shell.execute_reply.started":"2024-08-24T12:58:59.873306Z"},"trusted":true},"outputs":[],"source":["# tfidf_train_vectors_csr = csr_matrix(tfidf_train_vectors)\n","# train_length_features_csr = csr_matrix(train_length_features)\n","# combined_train_vectors = hstack([tfidf_train_vectors,  train_length_features_csr]) \n","# print(combined_train_vectors.shape)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.018311,"end_time":"2024-05-07T01:06:34.023615","exception":false,"start_time":"2024-05-07T01:06:34.005304","status":"completed"},"tags":[]},"source":["# Vectorize our test data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:58:59.891668Z","iopub.status.busy":"2024-08-24T12:58:59.891392Z","iopub.status.idle":"2024-08-24T12:59:00.308970Z","shell.execute_reply":"2024-08-24T12:59:00.308125Z","shell.execute_reply.started":"2024-08-24T12:58:59.891646Z"},"papermill":{"duration":0.127999,"end_time":"2024-05-07T01:06:34.177812","exception":false,"start_time":"2024-05-07T01:06:34.049813","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["print(\"Vectorizing test text...\")\n","\n","#our transformer\n","transformer_test_vectors = get_transformer_vectors(test)\n","\n","#get Word2Vec\n","word2vec_test_vectors = get_word2vec_vectors(test)\n","\n","#get TF-IDF\n","tfidf_test_vectors = get_tfidf_vectors(test)\n","\n","#Length features\n","test_length_features = get_length_features(test)\n","\n","#combine them!\n","transformer_test_vectors_csr = csr_matrix(transformer_test_vectors)  # Convert transformer vectors to a CSR matrix\n","word2vec_test_vectors_csr = csr_matrix(word2vec_test_vectors)  \n","test_length_features_csr = csr_matrix(test_length_features)\n","combined_test_vectors = hstack([tfidf_test_vectors, transformer_test_vectors_csr, word2vec_test_vectors_csr, test_length_features_csr]) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:59:00.310359Z","iopub.status.busy":"2024-08-24T12:59:00.310083Z","iopub.status.idle":"2024-08-24T12:59:00.314654Z","shell.execute_reply":"2024-08-24T12:59:00.313707Z","shell.execute_reply.started":"2024-08-24T12:59:00.310335Z"},"trusted":true},"outputs":[],"source":["# print(\"Vectorizing test text...\")\n","# #get TF-IDF\n","# tfidf_test_vectors = get_tfidf_vectors(test)\n","# #Length features\n","# test_length_features = get_length_features(test)\n","# #combine them!\n","# tfidf_test_vectors_csr = csr_matrix(tfidf_test_vectors)\n","# test_length_features_csr = csr_matrix(test_length_features)\n","# combined_test_vectors = hstack([tfidf_test_vectors_csr, test_length_features_csr]) \n","# print(\"Done!\")"]},{"cell_type":"markdown","metadata":{},"source":["# Train LightGBM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_filename = 'lightgbm_model.pkl'\n","\n","max_estimators = 1000\n","early_stopping_limit = 50\n","\n","# Data preparation\n","X = combined_train_vectors\n","# y = train[target_columns].idxmax(axis=1)\n","y_encoded = train['label'].values\n","\n","# Encode labels\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","# Split dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.05, random_state=42)\n","\n","# LightGBM parameters\n","params = {\n","    'n_estimators': max_estimators,\n","    'max_depth': 4,\n","    'subsample': 0.8,\n","    'colsample_bytree': 0.8,\n","    'objective': 'multiclass',\n","    'num_class': 3,\n","    'metric': 'multi_logloss',\n","    'random_state': 42,\n","    'learning_rate': 0.03,\n","    'verbose': -1  # keep logs quiet\n","}\n","\n","# Create the model\n","model = lgb.LGBMClassifier(**params)\n","\n","def callback(env):\n","    if env.iteration % 10 == 0: print (\"Iteration:\", env.iteration, \"\\tLog Loss:\", env.evaluation_result_list[0][2])\n","\n","model.fit(\n","    X_train, y_train,\n","    eval_set=[(X_test, y_test)],\n","    eval_metric='multi_logloss',    \n","    callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_limit), callback]  \n",")\n","\n","# Save the model to disk\n","joblib.dump(model, model_filename)\n","print(f\"Model saved to {model_filename}\")\n","\n","y_pred_proba = model.predict_proba(X_test)\n","\n","logloss = log_loss(y_test, y_pred_proba)\n","print(f\"\\nLog Loss: {logloss}\")\n","\n","y_pred = np.argmax(y_pred_proba, axis=1)  # Convert probabilities to class labels\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:59:00.316156Z","iopub.status.busy":"2024-08-24T12:59:00.315865Z","iopub.status.idle":"2024-08-24T12:59:00.325588Z","shell.execute_reply":"2024-08-24T12:59:00.324710Z","shell.execute_reply.started":"2024-08-24T12:59:00.316132Z"},"trusted":true},"outputs":[],"source":["# from sklearn.model_selection import StratifiedKFold\n","# from scipy.sparse import csr_matrix\n","# max_estimators = 1000\n","# early_stopping_limit = 50\n","\n","# # Data preparation\n","# X = combined_train_vectors\n","# y_encoded = train['label'].values\n","\n","# # LightGBM parameters\n","# params = {\n","#     'n_estimators': max_estimators,\n","#     'max_depth': 4,\n","#     'subsample': 0.8,\n","#     'colsample_bytree': 0.8,\n","#     'objective': 'multiclass',\n","#     'num_class': 3,\n","#     'metric': 'multi_logloss',\n","#     'random_state': 42,\n","#     'learning_rate': 0.03,\n","#     'verbose': -1  # keep logs quiet\n","# }\n","\n","# # Create the model\n","# model = lgb.LGBMClassifier(**params)\n","\n","# # 5-fold cross-validation\n","# stratified_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","# logloss_scores = []\n","# accuracy_scores = []\n","# test_pred_list = []\n","\n","# for fold, (train_indices, val_indices) in enumerate(stratified_k_fold.split(X, y_encoded)):\n","#     print(f\"\\nFold {fold + 1}\")\n","#     X_train_fold, X_val_fold = X[train_indices], X[val_indices]\n","#     y_train_fold, y_val_fold = y_encoded[train_indices], y_encoded[val_indices]\n","\n","#     def callback(env):\n","#         if env.iteration % 10 == 0: print (\"Iteration:\", env.iteration, \"\\tLog Loss:\", env.evaluation_result_list[0][2])\n","\n","#     model.fit(\n","#         X_train_fold, y_train_fold,\n","#         eval_set=[(X_val_fold, y_val_fold)],\n","#         eval_metric='multi_logloss',\n","#         callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_limit), callback]\n","#     )\n","\n","#     y_pred_proba_fold = model.predict_proba(X_val_fold)\n","#     logloss_fold = log_loss(y_val_fold, y_pred_proba_fold)\n","#     logloss_scores.append(logloss_fold)\n","#     print(f\"Log Loss: {logloss_fold}\")\n","    \n","#     y_pred_fold = np.argmax(y_pred_proba_fold, axis=1)\n","#     accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)\n","#     accuracy_scores.append(accuracy_fold)\n","#     print(f\"Accuracy: {accuracy_fold}\")\n","\n","#     test_pred_list.append(model.predict_proba(combined_test_vectors[-test.shape[0]:]))\n","\n","#     # Save the model to disk\n","#     model_filename = 'lightgbm_model'\n","#     model_filename_ = model_filename + f\"_fold_{fold + 1}.pkl\"\n","#     joblib.dump(model, model_filename_)\n","#     print(f\"\\nModel saved to {model_filename_}\")\n","\n","# # Calculate and print average scores\n","# average_logloss = np.mean(logloss_scores)\n","# average_accuracy = np.mean(accuracy_scores)\n","# print(f\"\\nAverage Log Loss: {average_logloss}\")\n","# print(f\"Average Accuracy: {average_accuracy}\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.027927,"end_time":"2024-05-07T01:06:34.236967","exception":false,"start_time":"2024-05-07T01:06:34.20904","status":"completed"},"tags":[]},"source":["# Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:59:32.457294Z","iopub.status.busy":"2024-08-24T12:59:32.456903Z","iopub.status.idle":"2024-08-24T12:59:32.464310Z","shell.execute_reply":"2024-08-24T12:59:32.463286Z","shell.execute_reply.started":"2024-08-24T12:59:32.457259Z"},"papermill":{"duration":0.106303,"end_time":"2024-05-07T01:06:34.373427","exception":false,"start_time":"2024-05-07T01:06:34.267124","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["preds_test = model.predict_proba(combined_test_vectors[-test.shape[0]:])\n","# preds_test = np.mean(test_pred_list, axis=0)\n","\n","# submission = pd.DataFrame({\n","#     'id': test[\"id\"],\n","#     'winner_model_a': preds_test[:, 0],\n","#     'winner_model_b': preds_test[:, 1], \n","#     'winner_tie': preds_test[:, 2]\n","# })\n","# submission.to_csv('submission.csv', index=False)\n","# print(submission)"]},{"cell_type":"markdown","metadata":{},"source":["# üß™ | Blend predictions of 2 models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:59:32.466292Z","iopub.status.busy":"2024-08-24T12:59:32.465820Z","iopub.status.idle":"2024-08-24T12:59:32.472932Z","shell.execute_reply":"2024-08-24T12:59:32.472172Z","shell.execute_reply.started":"2024-08-24T12:59:32.466246Z"},"trusted":true},"outputs":[],"source":["lgb_wt = 0.2\n","preds = lgb_wt * preds_test + (1 - lgb_wt) * gemma_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:59:32.478093Z","iopub.status.busy":"2024-08-24T12:59:32.477784Z","iopub.status.idle":"2024-08-24T12:59:32.484470Z","shell.execute_reply":"2024-08-24T12:59:32.483504Z","shell.execute_reply.started":"2024-08-24T12:59:32.478068Z"},"trusted":true},"outputs":[],"source":["preds"]},{"cell_type":"markdown","metadata":{},"source":["# üì¨ | Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T12:59:32.486167Z","iopub.status.busy":"2024-08-24T12:59:32.485786Z","iopub.status.idle":"2024-08-24T12:59:32.494841Z","shell.execute_reply":"2024-08-24T12:59:32.494003Z","shell.execute_reply.started":"2024-08-24T12:59:32.486135Z"},"trusted":true},"outputs":[],"source":["submission = pd.DataFrame({\n","    'id': test[\"id\"],\n","    'winner_model_a': preds[:, 0],\n","    'winner_model_b': preds[:, 1], \n","    'winner_tie': preds[:, 2]\n","})\n","submission.to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8346466,"sourceId":66631,"sourceType":"competition"},{"datasetId":2210196,"sourceId":3693646,"sourceType":"datasetVersion"},{"datasetId":4946449,"sourceId":8330401,"sourceType":"datasetVersion"},{"datasetId":5034873,"sourceId":8449074,"sourceType":"datasetVersion"},{"datasetId":5297895,"sourceId":8897601,"sourceType":"datasetVersion"},{"datasetId":5369301,"sourceId":8926343,"sourceType":"datasetVersion"},{"datasetId":5544839,"sourceId":9174887,"sourceType":"datasetVersion"},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":187768984,"sourceType":"kernelVersion"},{"isSourceIdPinned":true,"modelId":86587,"modelInstanceId":63082,"sourceId":75103,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":2185.862577,"end_time":"2024-05-07T01:06:39.269064","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-07T00:30:13.406487","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
